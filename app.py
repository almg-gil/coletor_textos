import os import re import time import glob import base64 import hashlib from datetime import datetime from io import BytesIO from typing import List, Tuple, Optional import pandas as pd import requests import streamlit as st from bs4 import BeautifulSoup # ================================================= # CONFIG # ================================================= st.set_page_config(page_title="Coletor de Textos ALMG (CSV por ano + API)", layout="wide") st.title("üìÑ Coletor de Textos ‚Äî ALMG (CSV por ano no GitHub)") st.caption( "L√™ automaticamente data_csv/LegislacaoMineira_YYYY.csv, baixa textos dos links " "(ex.: Dados Abertos tipoDoc=142/572) e gera CSV final com texto original + atualizado." ) DATA_DIR = "data_csv" FILE_PATTERN = os.path.join(DATA_DIR, "LegislacaoMineira_*.csv*") # .csv e .csv.gz REQUEST_TIMEOUT = 30 MIN_INTERVAL_SECONDS = 1.0 # respeitar 1 req/s (recomendado p/ dados abertos) SESSION = requests.Session() SESSION.headers.update({"User-Agent": "streamlit-almg-coletor/1.0"}) # Colunas do seu CSV (exatamente como voc√™ enviou) COL_TIPO = "Tipo" COL_NUMERO = "Numero" COL_ANO = "Ano" COL_LINK_ORIG = "LinkTextoOriginal" COL_LINK_ATU = "LinkTextoAtualizado" COL_FONTE = "Fonte" # vem como <a href="...">Publica√ß√£o</a> ... # ================================================= # UTIL # ================================================= def now_iso() -> str: return datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ") def limpar_texto(texto: str) -> str: texto = re.sub(r"[ \t]+", " ", texto or "") texto = re.sub(r"\n{3,}", "\n\n", texto) return texto.strip() def sha256_text(s: str) -> str: return hashlib.sha256((s or "").encode("utf-8", errors="ignore")).hexdigest() def url_portal(tipo: str, numero: int, ano: int) -> str: return f"https://www.almg.gov.br/legislacao-mineira/texto/{tipo}/{numero}/{ano}/" _last_req_ts = 0.0 def rate_limited_get(url: str) -> requests.Response: global _last_req_ts elapsed = time.time() - _last_req_ts if elapsed < MIN_INTERVAL_SECONDS: time.sleep(MIN_INTERVAL_SECONDS - elapsed) resp = SESSION.get(url, timeout=REQUEST_TIMEOUT) _last_req_ts = time.time() return resp def extract_first_href(html_snippet: str) -> str: """ Extrai o primeiro href de um trecho HTML tipo: <a href="...">Publica√ß√£o</a> """ if not isinstance(html_snippet, str) or not html_snippet.strip(): return "" m = re.search(r'href="([^"]+)"', html_snippet) return m.group(1) if m else "" def looks_like_base64(s: str) -> bool: if not s or len(s) < 200: return False return re.fullmatch(r"[A-Za-z0-9+/=\s]+", s) is not None def parse_api_text(obj) -> str: """ Extrai texto de um JSON em formatos variados. """ if isinstance(obj, str): return obj.strip() if not isinstance(obj, dict): return "" # chaves diretas comuns for k in ["conteudo", "texto", "html", "content", "documento", "body"]: v = obj.get(k) if isinstance(v, str) and len(v.strip()) > 20: return v.strip() # chaves comuns para base64 for k in ["conteudoBase64", "arquivoBase64", "base64", "pdfBase64", "binarioBase64"]: v = obj.get(k) if isinstance(v, str) and len(v.strip()) > 200: return v.strip() # recursivo em n√≥s comuns for k in ["dados", "data", "item", "resultado", "result"]: v = obj.get(k) if isinstance(v, dict): t = parse_api_text(v) if t: return t # listas for k in ["itens", "items", "resultados", "results"]: arr = obj.get(k) if isinstance(arr, list): for el in arr: if isinstance(el, dict): t = parse_api_text(el) if t: return t return "" def strip_html_and_decode_if_needed(texto: str) -> str: """ - Se for base64 e decodific√°vel em UTF-8, decodifica - Se tiver HTML, remove tags mantendo quebras """ if not texto: return "" texto = texto.strip() # base64 -> tenta decodificar em utf-8 (se for PDF bin√°rio, vai virar lixo; ent√£o aborta) if looks_like_base64(texto): try: decoded = base64.b64decode(texto, validate=False) decoded_txt = decoded.decode("utf-8", errors="ignore").strip() # se ficou muito pequeno, provavelmente era bin√°rio if len(decoded_txt) > 50: texto = decoded_txt except Exception: pass # HTML -> texto if "<" in texto and ">" in texto: soup = BeautifulSoup(texto, "html.parser") return limpar_texto(soup.get_text("\n", strip=True)) return limpar_texto(texto) def fetch_texto_por_link(url: str) -> Tuple[str, int, str]: """ Retorna (texto, status_code, debug) - Suporta JSON (Dados Abertos), HTML, base64 (quando decodific√°vel como texto) - Se retornar PDF/bin√°rio, volta vazio com debug informando content-type """ if not url or not isinstance(url, str) or not url.strip(): return "", 0, "url_vazia" try: r = rate_limited_get(url.strip()) status = r.status_code ctype = (r.headers.get("Content-Type") or "").lower() if status != 200: return "", status, f"status={status} ctype={ctype}" # JSON if "application/json" in ctype or "dadosabertos.almg.gov.br/api/" in url: try: j = r.json() raw = parse_api_text(j) txt = strip_html_and_decode_if_needed(raw) if not txt: keys = list(j.keys())[:25] if isinstance(j, dict) else [] return "", status, f"mode=json_sem_texto keys={keys}" return txt, status, "mode=json_ok" except Exception as e: # cai para HTML/texto pass # HTML/texto if "text/" in ctype or "html" in ctype: html = r.text or "" soup = BeautifulSoup(html, "html.parser") main = soup.find("main") or soup for tag in main.find_all(["nav", "header", "footer", "script", "style", "button", "aside"]): tag.decompose() txt = limpar_texto(main.get_text("\n", strip=True)) return txt, status, "mode=html_ok" # PDF/bin√°rio if "pdf" in ctype or "octet-stream" in ctype: return "", status, f"mode=binary({ctype})" # fallback txt = strip_html_and_decode_if_needed(r.text or "") return txt, status, "mode=fallback" except Exception as e: return "", 0, f"erro={e}" # ================================================= # CSV DO REPO # ================================================= @st.cache_data(show_spinner=False) def list_year_files() -> List[Tuple[int, str]]: files = sorted(glob.glob(FILE_PATTERN)) out = [] for f in files: m = re.search(r"LegislacaoMineira_(\d{4})\.csv(\.gz)?$", os.path.basename(f)) if m: out.append((int(m.group(1)), f)) out.sort(key=lambda x: x[0], reverse=True) return out def read_csv_smart(path: str) -> pd.DataFrame: """ Seu exemplo parece TAB-separated. Ent√£o: - tenta sep="\\t" - se n√£o der, tenta sep=";" - se n√£o der, tenta sep="," """ if path.endswith(".gz"): # tentativas com compress√£o for sep in ["\t", ";", ","]: try: df = pd.read_csv(path, compression="gzip", sep=sep, dtype=str, engine="python") if df.shape[1] >= 3: return df except Exception: pass else: for sep in ["\t", ";", ","]: try: df = pd.read_csv(path, sep=sep, dtype=str, engine="python") if df.shape[1] >= 3: return df except Exception: pass # √∫ltima tentativa padr√£o return pd.read_csv(path, dtype=str, engine="python") @st.cache_data(show_spinner=False) def load_year_csv(path: str) -> pd.DataFrame: return read_csv_smart(path) year_files = list_year_files() if not year_files: st.error(f"N√£o encontrei arquivos em {DATA_DIR} com o padr√£o LegislacaoMineira_YYYY.csv.") st.stop() anos_disponiveis = [y for y, _ in year_files] map_ano_arquivo = {y: p for y, p in year_files} colA, colB = st.columns([2, 1]) anos_sel = colA.multiselect("üìÖ Selecione ano(s)", options=anos_disponiveis, default=[anos_disponiveis[0]]) debug_on = colB.checkbox("Mostrar debug", value=True) if not anos_sel: st.stop() dfs = [load_year_csv(map_ano_arquivo[a]) for a in anos_sel] base = pd.concat(dfs, ignore_index=True) needed = {COL_TIPO, COL_NUMERO, COL_ANO, COL_LINK_ORIG} missing = [c for c in needed if c not in base.columns] if missing: st.error(f"Colunas ausentes no CSV: {missing}") st.stop() # normaliza base[COL_TIPO] = base[COL_TIPO].astype(str).str.upper().str.strip() base[COL_NUMERO] = pd.to_numeric(base[COL_NUMERO], errors="coerce") base[COL_ANO] = pd.to_numeric(base[COL_ANO], errors="coerce") base = base.dropna(subset=[COL_TIPO, COL_NUMERO, COL_ANO]) base[COL_NUMERO] = base[COL_NUMERO].astype(int) base[COL_ANO] = base[COL_ANO].astype(int) # garante coluna de link atualizado if COL_LINK_ATU not in base.columns: base[COL_LINK_ATU] = "" # extrai link de publica√ß√£o do HTML na coluna Fonte (se existir) if COL_FONTE in base.columns: base["LinkPublicacao"] = base[COL_FONTE].apply(extract_first_href) else: base["LinkPublicacao"] = "" # remove duplicatas por chave base = base.drop_duplicates(subset=[COL_TIPO, COL_NUMERO, COL_ANO]).reset_index(drop=True) st.markdown(f"‚úÖ Normas carregadas: **{len(base)}**") with st.expander("üëÄ Pr√©via do CSV (metadados)", expanded=False): st.dataframe(base.head(50), use_container_width=True) # ================================================= # MONTAR TABELA FINAL # ================================================= def montar_final(df: pd.DataFrame) -> pd.DataFrame: out = df.copy() out["tipo_sigla"] = out[COL_TIPO] out["numero"] = out[COL_NUMERO] out["ano"] = out[COL_ANO] out["url_portal"] = out.apply(lambda r: url_portal(r["tipo_sigla"], r["numero"], r["ano"]), axis=1) out["url_original"] = out[COL_LINK_ORIG].fillna("").astype(str) out["url_atualizado"] = out[COL_LINK_ATU].fillna("").astype(str) out["texto_original"] = "" out["texto_atualizado"] = "" out["status_original"] = "" out["status_atualizado"] = "" out["hash_original"] = "" out["hash_atualizado"] = "" out["coletado_em"] = "" if debug_on: out["debug_original"] = "" out["debug_atualizado"] = "" return out final = montar_final(base) # ================================================= # COLETA # ================================================= st.subheader("üöÄ Coletar textos (a partir dos links do CSV)") c1, c2, c3, c4 = st.columns([1.2, 1.2, 2, 1]) coletar_orig = c1.checkbox("Coletar original", value=True) coletar_atu = c2.checkbox("Coletar atualizado", value=True) limite = c3.number_input("Limite por execu√ß√£o", min_value=1, max_value=20000, value=min(300, len(final))) amostra = c4.checkbox("Amostra 30", value=False) if st.button("‚ñ∂Ô∏è Iniciar coleta"): if not coletar_orig and not coletar_atu: st.warning("Marque original e/ou atualizado.") st.stop() df = final.copy() if amostra: df = df.head(30).reset_index(drop=True) else: df = df.head(int(limite)).reset_index(drop=True) barra = st.progress(0) info = st.empty() t0 = time.time() for i, row in df.iterrows(): if coletar_orig: txt, code, dbg = fetch_texto_por_link(row["url_original"]) df.at[i, "texto_original"] = txt df.at[i, "status_original"] = str(code) df.at[i, "hash_original"] = sha256_text(txt) if txt else "" if debug_on: df.at[i, "debug_original"] = dbg if coletar_atu: # pode estar vazio mesmo (muitas normas n√£o t√™m consolidado) if row["url_atualizado"]: txt, code, dbg = fetch_texto_por_link(row["url_atualizado"]) df.at[i, "texto_atualizado"] = txt df.at[i, "status_atualizado"] = str(code) df.at[i, "hash_atualizado"] = sha256_text(txt) if txt else "" if debug_on: df.at[i, "debug_atualizado"] = dbg else: df.at[i, "status_atualizado"] = "url_vazia" if debug_on: df.at[i, "debug_atualizado"] = "url_vazia" df.at[i, "coletado_em"] = now_iso() if (i + 1) % 10 == 0: elapsed = time.time() - t0 info.write(f"Processadas **{i+1}/{len(df)}** | tempo: **{elapsed/60:.1f} min**") barra.progress((i + 1) / len(df)) st.success("‚úÖ Coleta finalizada para o lote.") st.dataframe(df.head(50), use_container_width=True) # Downloads buff = BytesIO() df.to_csv(buff, index=False, encoding="utf-8-sig") st.download_button( "‚¨áÔ∏è Baixar CSV final (com textos)", data=buff.getvalue(), file_name=f"LegislacaoMineira_{'_'.join(map(str, sorted(set(df['ano']))))}_com_textos.csv", mime="text/csv" ) buff_gz = BytesIO() df.to_csv(buff_gz, index=False, encoding="utf-8-sig", compression="gzip") st.download_button( "‚¨áÔ∏è Baixar CSV final (gzip)", data=buff_gz.getvalue(), file_name=f"LegislacaoMineira_{'_'.join(map(str, sorted(set(df['ano']))))}_com_textos.csv.gz", mime="application/gzip" ) else: st.caption("Dica: rode primeiro com **Amostra 30** para validar. Se texto_original continuar vazio, abra o debug_original na tabela.") st.dataframe(final.head(30), use_container_width=True)
